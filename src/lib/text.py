def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    # –ó–∞–º–µ–Ω—è–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã \r, \n, \t –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    text = text.replace('\r', ' ').replace('\n', ' ').replace('\t', ' ')

    # –ó–∞–º–µ–Ω–∞ —ë/–Å –Ω–∞ –µ/–ï
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ casefold (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è) —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    '''–ï—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –¥–µ–ª–∞—Ç—å –≤—Å–µ –±—É–∫–≤—ã –º–∞–ª–µ–Ω—å–∫–∏–º–∏, —Ç–æ —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å casefold(), –∫–æ—Ç–æ—Ä–∞—è –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ –±—É–∫–≤—ã –≤ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏ –¥–µ–ª–∞–µ—Ç —Å–ª–æ–≤–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏.
–ï—Å–ª–∏ –≤–¥—Ä—É–≥ —É –Ω–∞—Å –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π Python, –∏ casefold() –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Ç–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º lower() ‚Äî —Ç–æ–∂–µ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –±—É–∫–≤—ã –≤ –º–∞–ª–µ–Ω—å–∫–∏–µ.'''
    if casefold:
        try:
            text = text.casefold()
        except AttributeError:
            text = text.lower()

    # –°—Ö–ª–æ–ø—ã–≤–∞–µ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω –∏ —É–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
    result_chars = []#—Å—é–¥–∞ –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å –±—É–∫–≤—ã
    prev_space = False#—Ñ–ª–∞–∂–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –≥–æ–≤–æ—Ä–∏—Ç, –±—ã–ª –ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–∏–º–≤–æ–ª –ø—Ä–æ–±–µ–ª–æ–º.
    for ch in text:
        if ch.isspace():#–ï—Å–ª–∏ —Å–∏–º–≤–æ–ª ‚Äî —ç—Ç–æ –ø—Ä–æ–±–µ–ª –∏–ª–∏ –ø–æ—Ö–æ–∂–∏–π –∑–Ω–∞–∫ (isspace() –∑–Ω–∞—á–∏—Ç ‚Äî –ø—Ä–æ–±–µ–ª, —Ç–∞–±—É–ª—è—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏),
            if not prev_space:
                result_chars.append(' ')
                prev_space = True
        else:
            result_chars.append(ch)
            prev_space = False
    normalized_text = ''.join(result_chars).strip()
    return normalized_text


def tokenize(text: str) -> list[str]:
    tokens = []
    current_word = []

    def is_word_char(c):
        return c.isalnum() or c in ['_', '-']

    for c in text:
        if is_word_char(c):
            current_word.append(c)
        elif c == '-':
            # –¥–µ—Ñ–∏—Å –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ –µ—Å—Ç—å
            if current_word:
                current_word.append(c)
        else:
            # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            if current_word:
                tokens.append(''.join(current_word))
                current_word = []
    if current_word:
        tokens.append(''.join(current_word))
    return tokens


def count_freq(tokens: list[str]) -> dict:
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq


def top_n(freq: dict, n: int = 5) -> list:
    return sorted(freq.items(), key=lambda item: (-item[1], item[0]))[:n]

#—Ç–µ—Å—Ç—ã

text = "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"
text1 = '—ë–∂–∏–∫, –Å–ª–∫–∞'
text2 = 'Hello\r\nWorld'
text3 = '  –¥–≤–æ–π–Ω—ã–µ  –ø—Ä–æ–±–µ–ª—ã  '
norm = normalize(text)
norm1 = normalize(text1)
norm2 = normalize(text2)
norm3 = normalize(text3)
print(norm)  
print(norm1)
print(norm2)
print(norm3)


stroka = '–ø—Ä–∏–≤–µ—Ç –º–∏—Ä'
stroka1 = 'hello,world!!!'
stroka2 = '–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ'
stroka3 = '2025 –≥–æ–¥'
stroka4 = 'emoji üòÉ –Ω–µ —Å–ª–æ–≤–æ'

tokens = tokenize(stroka)
tokens1 = tokenize(stroka1)
tokens2 = tokenize(stroka2)
tokens3 = tokenize(stroka3)
tokens4 = tokenize(stroka4)
print(tokens) 
print(tokens1)
print(tokens2)
print(tokens3)
print(tokens4)

test = ['a', 'b', 'a', 'c', 'b', 'a']
test1 = ['bb', 'aa', 'bb', 'aa', 'cc']
freq = count_freq(test)
print(freq)

freq1 = count_freq(test1)
top = top_n(freq1)
print(top)